# 2019

## NeurIPS
- Post-training 4-bit quantization of convolution networks for rapid-deployment
- Efficient and Effective Quantization for Sparse DNNs
- Einconv: Exploring Unexplored Tensor Decompositions for Convolutional Neural Networks

## CVPR

- Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss
- Simultaneously Optimizing Weight and Quantizer of Ternary Neural Network using Truncated Gaussian Approximation
- Structured Pruning of Neural Networks with Budget-Aware Regularization
- Towards Optimal Structured CNN Pruning via Generative Adversarial Learning
- Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure
- Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration
- ECC: Platform-Independent Energy-Constrained Deep Neural Network Compression via a Bilinear Regression Model
- Cascaded Projection: End-to-End Network Compression and Acceleration
- Accelerating Convolutional Neural Networks via Activation Map Compression
- Energy-Constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking
- Factorized Convolutional Neural Networks
- Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression
- A Main/Subsidiary Network Framework for Simplifying Binary Neural Networks
- Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?
- Cross Domain Model Compression by Structurally Weight Sharing

## ICML

- Improving Neural Network Quantization without Retraining using Outlier Channel Splitting
- Same, Same But Different-Recovering Neural Network Quantization Error Through Weight Factorization
- Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization
- Variational inference for sparse network reconstruction from count data


# 2018

## NeurIPS

* Scalable Methods for 8-bit Training of Neural Networks
* Heterogeneous Bitwidth Binarization in Convolutional Neural Networks
* HitNet: Hybrid Ternary Recurrent Neural Network

## ICML
- WSNet: Compact and Efficient Networks Through Weight Sampling

## ICLR

* Espresso: Efficient Forward Propagation for BCNNs
* An Empirical study of Binary Neural Networks' Optimisation
* [Learning Discrete Weights Using the Local Reparameterization Trick](https://arxiv.org/abs/1710.07739)
* On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks
* Learning To Share: Simultaneous Parameter Tying and Sparsification in Deep Learning

## ECCV

* Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm
* Value-aware Quantization for Training and Inference of Neural Networks
* LSQ++: Lower running time and higher recall in multi-codebook quantization
* LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks



## CVPR ##

- NISP: Pruning Networks using Neuron Importance Score Propagation
- SYQ: Learning Symmetric Quantization For Efficient Deep Neural Networks



## IJCAI ##

- Improving Deep Neural Network Sparsity through Decorrelation Regularization



# 2016

## ICML

* Fixed Point Quantization of Deep Convolutional Networks
* 





# 2014

## NIPS

* Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights