# General

## Weights Quantization

* BinaryConnect: Training Deep Neural Networks with binary weights during propagations
* Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM
* [Loss-aware Binarization of Deep Networks](https://arxiv.org/abs/1611.01600)
* [Loss-aware Weight Quantization of Deep Networks](https://arxiv.org/abs/1802.08635)
* Overcoming Challenges in Fixed Point Training of Deep Convolutional Networks
* CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization
* Two-Step Quantization for Low-bit Neural Networks
* [Training deep neural networks with low precision multiplications](https://arxiv.org/abs/1412.7024)
* Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks
* Value-aware Quantization for Training and Inference of Neural Networks
* Training Competitive Binary Neural Networks from Scratch

## Weights Ternarization

- Trained Ternary Quantization

## Weights & Activation Quantization

* Quantized Neural Networks Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations
* Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1
* XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks
* Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm
* LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks

## Weights & Activation & Error & Gradient  Quantization

* DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients
* Training and Inference with Integers in Deep Neural Networks

## Non-symmetric Quantization

* Weighted-Entropy-based Quantization for Deep Neural Networks
* Towards the Limit of Network Quantization
* LSQ++: Lower running time and higher recall in multi-codebook quantization

## Unclassified

* Scalable Methods for 8-bit Training of Neural Networks
* Efficient end-to-end learning for quantizable representations ([code](https://github.com/maestrojeong/Deep-Hash-Table-ICML18)) !
* Network Sketching: Exploiting Binary Structure in Deep CNNs !
* DNQ: Dynamic Network Quantization
* Training Hard-Threshold Networks with Combinatorial Search in a Discrete Target Propagation Setting
* An Empirical study of Binary Neural Networks' Optimisation !
* Heterogeneous Bitwidth Binarization in Convolutional Neural Networks !
* HitNet: Hybrid Ternary Recurrent Neural Network !



# Probabilistic (Bayesian) Quantization

* Relaxed Quantization for Discretized Neural Networks
* [Learning Discrete Weights Using the Local Reparameterization Trick](https://arxiv.org/abs/1710.07739)
* Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights



# Efficient Model Design

- EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
- ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices
- ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design
- MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
- MobileNetV2: Inverted Residuals and Linear Bottlenecks
- SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND <0.5MB MODEL SIZE
- SqueezeNext: Hardware-Aware Neural Network Design
- Xception: Deep Learning with Depthwise Separable Convolutions



# Quantization Theory

* Analysis of Quantized Models
* On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks



# Pruning Theory

- Stronger generalization bounds for deep nets via a compression approach



# Quantization with Distillation

* [Model compression via distillation and quantization](https://arxiv.org/abs/1802.05668)



# Adaptive Quantization

* [Adaptive Quantization of Neural Networks](https://openreview.net/forum?id=SyOK1Sg0W)



# Gradient Quantization

* SIGNSGD: compressed optimisation for non-convex problems
* QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding



# Relaxed Quantization

* BinaryRelax: A Relaxation Approach For Training Deep Neural Networks With Quantized Weights



# Specified Application

* Quantized Convolutional Neural Networks for Mobile Devices



# Hardware Prespective

* Espresso: Efficient Forward Propagation for BCNNs



# Structure Pruning

- Structured Pruning of Neural Networks with Budget-Aware Regularization

- Towards Optimal Structured CNN Pruning via Generative Adversarial Learning

- Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression

- Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration

  - Is pruning by absolute value meaningful?

  

# Element-wise Pruning



# Network Factorization



# Activation / Feature Compression

- Accelerating Convolutional Neural Networks via Activation Map Compression
- 

