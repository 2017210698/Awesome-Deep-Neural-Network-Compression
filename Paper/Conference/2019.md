# NeurIPS

- Efficient and Effective Quantization for Sparse DNNs
- Focused Quantization for Sparse CNNs [[paper]](https://papers.nips.cc/paper/8796-focused-quantization-for-sparse-cnns.pdf)
- Point-Voxel CNN for Efficient 3D Deep Learning [[paper]](https://arxiv.org/abs/1907.03739)
- Model Compression with Adversarial Robustness: A Unified Optimization Framework [[paper]](https://arxiv.org/abs/1902.03538)

## Quantization
- MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable Quantization [[paper]](https://github.com/csyhhu/MetaQuant/blob/master/MetaQuant-Preprint.pdf) [[codes]](https://github.com/csyhhu/MetaQuant)
- Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization [[paper]](https://papers.nips.cc/paper/8971-latent-weights-do-not-exist-rethinking-binarized-neural-network-optimization.pdf)

### Post Quantization
- Post-training 4-bit quantization of convolution networks for rapid-deployment


### Gradient Compression
- Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification, and Local Computations [[paper]](https://arxiv.org/pdf/1906.02367.pdf)
- PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization [[paper]](https://arxiv.org/pdf/1905.13727.pdf)


## Pruning
- AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters


### Unstructure Pruning
- Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask
- Global Sparse Momentum SGD for Pruning Very
Deep Neural Networks [[paper]](https://arxiv.org/pdf/1909.12778.pdf)[[codes]](https://github.com/DingXiaoH/GSM-SGD)

### Structrue Pruning
- Channel Gating Neural Network
- Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks

## Distillation
- Positive-Unlabeled Compression on the Cloud [[paper]](https://arxiv.org/abs/1909.09757)


## Factorization
- Einconv: Exploring Unexplored Tensor Decompositions for Convolutional Neural Networks [[paper]](https://arxiv.org/abs/1908.04471) [[codes]](https://github.com/pfnet-research/einconv)
- A Tensorized Transformer for Language Modeling [[paper]](https://arxiv.org/pdf/1906.09777.pdf)


## Efficient Model Design
- Shallow RNN: Accurate Time-series Classification on Resource Constrained Devices [[paper]](https://papers.nips.cc/paper/9451-shallow-rnn-accurate-time-series-classification-on-resource-constrained-devices.pdf)
- CondConv: Conditionally Parameterized Convolutions for Efficient Inference [[paper]](https://papers.nips.cc/paper/8412-condconv-conditionally-parameterized-convolutions-for-efficient-inference.pdf)


## Dynamic Inference
- SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models [[paper]](https://papers.nips.cc/paper/8657-scan-a-scalable-neural-networks-framework-towards-compact-and-efficient-models.pdf)


## Neural Architecture Search
- Constrained deep neural network architecture search for IoT devices accounting hardware calibration [[paper]](https://papers.nips.cc/paper/8838-constrained-deep-neural-network-architecture-search-for-iot-devices-accounting-for-hardware-calibration.pdf)
- DATA: Differentiable ArchiTecture Approximation [[paper]](https://papers.nips.cc/paper/8374-data-differentiable-architecture-approximation.pdf)
- Efficient Forward Architecture Search [[paper]](https://arxiv.org/pdf/1905.13360.pdf)

## Cost (Energy, Memory, Time) Saving Training
- Hybrid 8-bit Floating Point (HFP8) Training and
Inference for Deep Neural Networks [[paper]](https://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks.pdf)
- E2-Train: Training State-of-the-art CNNs with Over
80% Energy Savings [[paper]](https://arxiv.org/pdf/1910.13349.pdf)
- Backprop with Approximate Activations for Memory-efficient Network Training [[paper]](https://arxiv.org/pdf/1901.07988.pdf)


## Theory
- Dimension-Free Bounds for Low-Precision Training [[paper]](https://papers.nips.cc/paper/9346-dimension-free-bounds-for-low-precision-training.pdf)
- A Mean Field Theory of Quantized Deep Networks:
The Quantization-Depth Trade-Off [[paper]](https://arxiv.org/pdf/1906.00771.pdf)


# CVPR


# ICCV


# ICML