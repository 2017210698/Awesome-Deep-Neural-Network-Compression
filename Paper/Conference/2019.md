# NeurIPS

- Efficient and Effective Quantization for Sparse DNNs
- Focused Quantization for Sparse CNNs [[paper]](https://papers.nips.cc/paper/8796-focused-quantization-for-sparse-cnns.pdf)
- Point-Voxel CNN for Efficient 3D Deep Learning [[paper]](https://arxiv.org/abs/1907.03739)
- Model Compression with Adversarial Robustness: A Unified Optimization Framework [[paper]](https://arxiv.org/abs/1902.03538)

## Quantization
- MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable Quantization [[paper]](https://github.com/csyhhu/MetaQuant/blob/master/MetaQuant-Preprint.pdf) [[codes]](https://github.com/csyhhu/MetaQuant)
- Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization [[paper]](https://papers.nips.cc/paper/8971-latent-weights-do-not-exist-rethinking-binarized-neural-network-optimization.pdf)

### Post Quantization
- Post-training 4-bit quantization of convolution networks for rapid-deployment


### Gradient Compression
- Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification, and Local Computations [[paper]](https://arxiv.org/pdf/1906.02367.pdf)
- PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization [[paper]](https://arxiv.org/pdf/1905.13727.pdf)


## Pruning
- AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters
- Network Pruning via Transformable Architecture Search

### Unstructure Pruning
- Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask
- Global Sparse Momentum SGD for Pruning Very
Deep Neural Networks [[paper]](https://arxiv.org/pdf/1909.12778.pdf)[[codes]](https://github.com/DingXiaoH/GSM-SGD)

### Structrue Pruning
- Channel Gating Neural Network
- Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks

## Distillation
- Positive-Unlabeled Compression on the Cloud [[paper]](https://arxiv.org/abs/1909.09757)

## Factorization
- Einconv: Exploring Unexplored Tensor Decompositions for Convolutional Neural Networks [[paper]](https://arxiv.org/abs/1908.04471) [[codes]](https://github.com/pfnet-research/einconv)
- A Tensorized Transformer for Language Modeling [[paper]](https://arxiv.org/pdf/1906.09777.pdf)


## Efficient Model Design
- Shallow RNN: Accurate Time-series Classification on Resource Constrained Devices [[paper]](https://papers.nips.cc/paper/9451-shallow-rnn-accurate-time-series-classification-on-resource-constrained-devices.pdf)
- CondConv: Conditionally Parameterized Convolutions for Efficient Inference [[paper]](https://papers.nips.cc/paper/8412-condconv-conditionally-parameterized-convolutions-for-efficient-inference.pdf)


## Dynamic Inference
- SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models [[paper]](https://papers.nips.cc/paper/8657-scan-a-scalable-neural-networks-framework-towards-compact-and-efficient-models.pdf)


## Neural Architecture Search
- Constrained deep neural network architecture search for IoT devices accounting hardware calibration [[paper]](https://papers.nips.cc/paper/8838-constrained-deep-neural-network-architecture-search-for-iot-devices-accounting-for-hardware-calibration.pdf)
- DATA: Differentiable ArchiTecture Approximation [[paper]](https://papers.nips.cc/paper/8374-data-differentiable-architecture-approximation.pdf)
- Efficient Forward Architecture Search [[paper]](https://arxiv.org/pdf/1905.13360.pdf)

## Cost (Energy, Memory, Time) Saving Training
- Hybrid 8-bit Floating Point (HFP8) Training and
Inference for Deep Neural Networks [[paper]](https://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks.pdf)
- E2-Train: Training State-of-the-art CNNs with Over
80% Energy Savings [[paper]](https://arxiv.org/pdf/1910.13349.pdf)
- Backprop with Approximate Activations for Memory-efficient Network Training [[paper]](https://arxiv.org/pdf/1901.07988.pdf)


## Theory
- Dimension-Free Bounds for Low-Precision Training [[paper]](https://papers.nips.cc/paper/9346-dimension-free-bounds-for-low-precision-training.pdf)
- A Mean Field Theory of Quantized Deep Networks:
The Quantization-Depth Trade-Off [[paper]](https://arxiv.org/pdf/1906.00771.pdf)


# CVPR

- Enhanced Bayesian Compression via Deep Reinforcement Learning
- Accelerating Convolutional Neural Networks via Activation Map Compression
- Cross Domain Model Compression by Structurally Weight Sharing
- Cascaded Projection: End-To-End Network Compression and Acceleration
- Efficient Neural Network Compression

## Quantization
- Fully Quantized Network for Object Detection
- Learning to Quantize Deep Networks by Optimizing Quantization Intervals With Task Loss
- Quantization Networks
- SeerNet: Predicting Convolutional Neural Network Feature-Map Sparsity Through Low-Bit Quantization
- Simultaneously Optimizing Weight and Quantizer of Ternary Neural Network Using Truncated Gaussian Approximation
- Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?
- A Main/Subsidiary Network Framework for Simplifying Binary Neural Networks
- Regularizing Activation Distribution for Training Binarized Deep Networks
- Structured Binary Neural Networks for Accurate Image Classification and Semantic Segmentation
- Learning Channel-Wise Interactions for Binary Convolutional Neural Networks
- Circulant Binary Convolutional Networks: Enhancing the Performance of 1-Bit DCNNs With Circulant Back Propagation


## Pruning
- Variational Convolutional Neural Network Pruning
- Importance Estimation for Neural Network Pruning
- Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression

### Strcture Pruning
- Towards Optimal Structured CNN Pruning via Generative Adversarial Learning
- Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration
- Centripetal SGD for Pruning Very Deep Convolutional Networks With Complicated Structure
- Structured Pruning of Neural Networks With Budget-Aware Regularization


## Co-Design (Software-Hardware-Energy)
- HAQ: Hardware-Aware Automated Quantization With Mixed Precision
- ECC: Platform-Independent Energy-Constrained Deep Neural Network Compression via a Bilinear Regression Model


## Factorization
- Compressing Convolutional Neural Networks via Factorized Convolutional Filters


## Accleration
- Fully Learnable Group Convolution for Acceleration of Deep Neural Networks


# ICCV
- Learning Filter Basis for Convolutional Neural Network Compression

## Quantization
- Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks
- Proximal Mean-Field for Neural Network Quantization

### Data-Free/Post-Quantization
- Data-Free Quantization Through Weight Equalization and Bias Correction


## Pruning
- Accelerate CNN via Recursive Bayesian Pruning
- A Bayesian Optimization Framework for Neural Network Compression

### Structure Pruning
- MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning

## Co-Design (Software-Hardware-Energy)
- HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision

## Distillation
- Knowledge Distillation via Route Constrained Optimization
- Learning Lightweight Lane Detection CNNs by Self Attention Distillation
- Online Model Distillation for Efficient Video Inference


## Adversarial / Robustness
- Adversarial Robustness vs. Model Compression, or Both?


# ICML

## Quantization
### Post/Data-Free Quantization
- Improving Neural Network Quantization without Retraining using Outlier Channel Splitting
- Same, Same But Different: Recovering Neural Network Quantization Error Through Weight Factorization

## Pruning
### Structure Pruning
- Approximated Oracle Filter Pruning for Destructive CNN Width Optimization
- Collaborative Channel Pruning for Deep Networks
- EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis


## Distillation
- LIT: Learned Intermediate Representation Training for Model Compression
- Zero-Shot Knowledge Distillation in Deep Networks
- Towards Understanding Knowledge Distillation
## Theory
- Rate Distortion For Model Compression: From Theory To Practice


# ICLR
- Learnable Embedding Space for Efficient Neural Architecture Compression
- Minimal Random Code Learning: Getting Bits Back from Compressed Model Parameters

## Pruning
- The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks
- Rethinking the Value of Network Pruning 
- SNIP: Single-shot Network Pruning based on Connection Sensitivity

### Structure Pruning
- Dynamic Channel Pruning: Feature Boosting and Suppression

## Quantization
- Relaxed Quantization for Discretized Neural Networks
- ProxQuant: Quantized Neural Networks via Proximal Operators 
- Per-Tensor Fixed-Point Quantization of the Back-Propagation Algorithm
- Defensive Quantization: When Efficiency Meets Robustness
- Learning Recurrent Binary/Ternary Weights 
- Combinatorial Attacks on Binarized Neural Networks
- ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks 
- An Empirical study of Binary Neural Networks' Optimisation 
- Integer Networks for Data Compression with Latent-Variable Models 


## Co-Design
- Double Viterbi: Weight Encoding for High Compression Ratio and Fast On-Chip Reconstruction for Deep Neural Network 
- Energy-Constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking 

## Theory
- On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks 
- Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets
- Analysis of Quantized Models
- Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds